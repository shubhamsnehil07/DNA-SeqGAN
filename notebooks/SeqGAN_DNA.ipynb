{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **DNA Sequences Generation using SeqGAN**"
      ],
      "metadata": {
        "id": "oYcWFws35JjY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importing Dependencies**"
      ],
      "metadata": {
        "id": "bAOlFsqR8Ye1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.distributions import Categorical\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "wc07hUl55mNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Constants**"
      ],
      "metadata": {
        "id": "Yr1W0cED5p_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "SEQ_LENGTH = 56\n",
        "VOCAB_SIZE = 4  # A, C, G, T\n",
        "HIDDEN_DIM = 128\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 100\n",
        "G_PRETRAIN_EPOCHS = 50\n",
        "D_PRETRAIN_EPOCHS = 50"
      ],
      "metadata": {
        "id": "vRaDzTuv5oqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Nucleotide mapping**"
      ],
      "metadata": {
        "id": "AQDbLgZk51mH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nucleotide mapping\n",
        "NUCLEOTIDE_MAP = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
        "INV_NUCLEOTIDE_MAP = {v: k for k, v in NUCLEOTIDE_MAP.items()}"
      ],
      "metadata": {
        "id": "QCePy0MI5zV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load and preprocess data**"
      ],
      "metadata": {
        "id": "55AEW6Ep8RH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path, header=None, names=['label', 'name', 'sequence'])\n",
        "    sequences = df['sequence'].str.upper().tolist()\n",
        "    # Convert sequences to integer indices\n",
        "    seq_indices = [[NUCLEOTIDE_MAP[c] for c in seq if c in NUCLEOTIDE_MAP] for seq in sequences]\n",
        "    # Filter sequences of correct length\n",
        "    seq_indices = [seq for seq in seq_indices if len(seq) == SEQ_LENGTH]\n",
        "    # One-hot encoding\n",
        "    one_hot = np.zeros((len(seq_indices), SEQ_LENGTH, VOCAB_SIZE))\n",
        "    for i, seq in enumerate(seq_indices):\n",
        "        for j, idx in enumerate(seq):\n",
        "            one_hot[i, j, idx] = 1\n",
        "    return torch.tensor(one_hot, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "dkdvuKnH59rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "SEQ_LENGTH = 56\n",
        "VOCAB_SIZE = 4  # A, C, G, T\n",
        "NUCLEOTIDE_MAP = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Load and preprocess DNA sequences from a CSV file.\n",
        "    Args:\n",
        "        file_path (str): Path to the CSV file.\n",
        "    Returns:\n",
        "        torch.Tensor: One-hot encoded sequences.\n",
        "    Raises:\n",
        "        ValueError: If no valid sequences are found or the file is malformed.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read CSV with flexible delimiter handling\n",
        "        df = pd.read_csv(file_path, header=None, names=['label', 'name', 'sequence'])\n",
        "        print(f\"Loaded {len(df)} sequences from {file_path}\")\n",
        "\n",
        "        # Removing the tab '\\t'\n",
        "        seq = df['sequence'].str.upper().str.strip().tolist()\n",
        "        new_seq = []\n",
        "        for i in range(0, len(seq)):\n",
        "          new_seq.append(seq[i][1:])\n",
        "\n",
        "        new_seq_final = []\n",
        "        for i in range(0, len(new_seq)):\n",
        "          if new_seq[i][0] == '\\t':\n",
        "            new_seq_final.append(new_seq[i][1:])\n",
        "          else:\n",
        "            new_seq_final.append(new_seq[i])\n",
        "\n",
        "        sequences = new_seq_final\n",
        "\n",
        "        # Convert sequences to uppercase and strip whitespace\n",
        "        print(f\"Raw sequences count: {len(sequences)}\")\n",
        "\n",
        "        # Convert sequences to integer indices and filter invalid sequences\n",
        "        seq_indices = []\n",
        "        invalid_chars = set()\n",
        "        skipped_sequences = 0\n",
        "        for i, seq in enumerate(sequences):\n",
        "            # Check if sequence is valid (not None and is a string)\n",
        "            if not isinstance(seq, str):\n",
        "                print(f\"Skipping sequence at index {i}: Not a string (value: {seq})\")\n",
        "                skipped_sequences += 1\n",
        "                continue\n",
        "            try:\n",
        "                indices = [NUCLEOTIDE_MAP[c] for c in seq if c in NUCLEOTIDE_MAP]\n",
        "                # Only include sequences with valid length\n",
        "                if len(indices) == SEQ_LENGTH:\n",
        "                    seq_indices.append(indices)\n",
        "                else:\n",
        "                    print(f\"Skipping sequence at index {i}: Length {len(indices)} (expected {SEQ_LENGTH}): {seq[:10]}...\")\n",
        "                    skipped_sequences += 1\n",
        "            except KeyError as e:\n",
        "                invalid_chars.add(str(e))\n",
        "                print(f\"Skipping sequence at index {i}: Invalid character {str(e)} in sequence: {seq[:10]}...\")\n",
        "                skipped_sequences += 1\n",
        "\n",
        "        if invalid_chars:\n",
        "            print(f\"Invalid characters found: {invalid_chars}\")\n",
        "        if skipped_sequences > 0:\n",
        "            print(f\"Skipped {skipped_sequences} invalid or incorrect-length sequences\")\n",
        "\n",
        "        # Check if any valid sequences remain\n",
        "        if not seq_indices:\n",
        "            raise ValueError(f\"No valid sequences found after filtering. Check input file for valid DNA sequences.\")\n",
        "        print(f\"Valid sequences after filtering: {len(seq_indices)}\")\n",
        "\n",
        "        # One-hot encoding\n",
        "        one_hot = np.zeros((len(seq_indices), SEQ_LENGTH, VOCAB_SIZE), dtype=np.float32)\n",
        "        for i, seq in enumerate(seq_indices):\n",
        "            for j, idx in enumerate(seq):\n",
        "                one_hot[i, j, idx] = 1\n",
        "\n",
        "        # Convert to torch tensor\n",
        "        tensor_data = torch.tensor(one_hot, dtype=torch.float32)\n",
        "        print(f\"One-hot encoded tensor shape: {tensor_data.shape}\")\n",
        "\n",
        "        return tensor_data\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"Could not find file: {file_path}\")\n",
        "    except pd.errors.EmptyDataError:\n",
        "        raise ValueError(f\"File {file_path} is empty or malformed\")\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Error processing file {file_path}: {str(e)}\")"
      ],
      "metadata": {
        "id": "Da6saXlX9miD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Generator model**"
      ],
      "metadata": {
        "id": "bOe9oZ998NEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator model\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.lstm = nn.LSTM(VOCAB_SIZE, HIDDEN_DIM, batch_first=True)\n",
        "        self.fc = nn.Linear(HIDDEN_DIM, VOCAB_SIZE)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        output, hidden = self.lstm(x, hidden)\n",
        "        output = self.fc(output)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden"
      ],
      "metadata": {
        "id": "xIH0vi356Aep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Discriminator model**"
      ],
      "metadata": {
        "id": "bH9Dcne-6Dwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Discriminator model\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(VOCAB_SIZE, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(2)\n",
        "        self.fc1 = nn.Linear(128 * 14, 256)\n",
        "        self.fc2 = nn.Linear(256, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)  # (batch, seq_len, vocab) -> (batch, vocab, seq_len)\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.sigmoid(self.fc2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "WX-9ydi76ET_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Monte Carlo rollout for reward estimation**"
      ],
      "metadata": {
        "id": "Cja0huKz6Fom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Monte Carlo rollout for reward estimation\n",
        "def rollout(generator, seq, num_rollouts=10):\n",
        "    rewards = []\n",
        "    for _ in range(num_rollouts):\n",
        "        current_seq = seq.clone()\n",
        "        hidden = None\n",
        "        for t in range(SEQ_LENGTH):\n",
        "            if t < len(seq[0]):\n",
        "                input_t = torch.zeros(seq.size(0), 1, VOCAB_SIZE).to(seq.device)\n",
        "                input_t.scatter_(2, seq[:, t:t+1].unsqueeze(-1), 1)\n",
        "            else:\n",
        "                input_t = torch.zeros(seq.size(0), 1, VOCAB_SIZE).to(seq.device)\n",
        "                input_t[:, :, 0] = 1  # Default to A\n",
        "            probs, hidden = generator(input_t, hidden)\n",
        "            dist = Categorical(probs.squeeze(1))\n",
        "            next_nucleotide = dist.sample().unsqueeze(1)\n",
        "            current_seq = torch.cat([current_seq[:, :t], next_nucleotide], dim=1)\n",
        "        # Convert to one-hot for discriminator\n",
        "        one_hot = torch.zeros(seq.size(0), SEQ_LENGTH, VOCAB_SIZE).to(seq.device)\n",
        "        one_hot.scatter_(2, current_seq.unsqueeze(-1), 1)\n",
        "        reward = discriminator(one_hot).detach()\n",
        "        rewards.append(reward)\n",
        "    return torch.mean(torch.stack(rewards), dim=0)"
      ],
      "metadata": {
        "id": "0VvKvsDF6GE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading Data**"
      ],
      "metadata": {
        "id": "A42gSUq16TOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading data\n",
        "data = load_data('data.csv')\n",
        "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "azX24YCU6Tbl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56dd824c-2446-4aa0-9fa0-265b71f0566a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 106 sequences from data.csv\n",
            "Raw sequences count: 106\n",
            "Valid sequences after filtering: 106\n",
            "One-hot encoded tensor shape: torch.Size([106, 56, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Initializing models**"
      ],
      "metadata": {
        "id": "SwpxWvyl6czW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing models\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "g_optimizer = optim.Adam(generator.parameters(), lr=0.001)\n",
        "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.001)\n",
        "criterion = nn.BCELoss()\n",
        "cross_entropy = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "swLXsujq6c-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pretrain generator with MLE**"
      ],
      "metadata": {
        "id": "roDMnpyb72wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretrain generator with MLE\n",
        "for epoch in range(G_PRETRAIN_EPOCHS):\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    for i in range(0, len(train_data), BATCH_SIZE):\n",
        "        batch = train_data[i:i+BATCH_SIZE]\n",
        "        g_optimizer.zero_grad()\n",
        "        input_seq = batch[:, :-1]  # [batch_size, 56, 4]\n",
        "        target_seq = batch[:, 1:]  # [batch_size, 56, 4]\n",
        "        target_indices = torch.argmax(target_seq, dim=-1)  # [batch_size, 56]\n",
        "        probs, _ = generator(input_seq)  # [batch_size, 56, 4]\n",
        "        # Clip probabilities to avoid log(0)\n",
        "        probs = torch.clamp(probs, min=1e-10, max=1.0)\n",
        "        # Use CrossEntropyLoss\n",
        "        loss = cross_entropy(probs.view(-1, VOCAB_SIZE), target_indices.view(-1))\n",
        "        loss.backward()\n",
        "        # Clip gradients to prevent explosion\n",
        "        torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
        "        g_optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "    avg_loss = total_loss / num_batches if num_batches > 0 else float('nan')\n",
        "    print(f'Generator Pretrain Epoch {epoch+1}, Loss: {avg_loss}')"
      ],
      "metadata": {
        "id": "pkTSwwD16jq3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8858364-8d3e-405a-d1ce-7421e478410c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Pretrain Epoch 1, Loss: 1.3873674074808757\n",
            "Generator Pretrain Epoch 2, Loss: 1.3863003651301067\n",
            "Generator Pretrain Epoch 3, Loss: 1.3852418263753254\n",
            "Generator Pretrain Epoch 4, Loss: 1.3840083678563435\n",
            "Generator Pretrain Epoch 5, Loss: 1.382477839787801\n",
            "Generator Pretrain Epoch 6, Loss: 1.3823171059290569\n",
            "Generator Pretrain Epoch 7, Loss: 1.3816022078196208\n",
            "Generator Pretrain Epoch 8, Loss: 1.3815967241923015\n",
            "Generator Pretrain Epoch 9, Loss: 1.3816067377726238\n",
            "Generator Pretrain Epoch 10, Loss: 1.381461461385091\n",
            "Generator Pretrain Epoch 11, Loss: 1.3812662760416667\n",
            "Generator Pretrain Epoch 12, Loss: 1.3811468680699666\n",
            "Generator Pretrain Epoch 13, Loss: 1.3810993830362956\n",
            "Generator Pretrain Epoch 14, Loss: 1.380990982055664\n",
            "Generator Pretrain Epoch 15, Loss: 1.3808532158533733\n",
            "Generator Pretrain Epoch 16, Loss: 1.3807512521743774\n",
            "Generator Pretrain Epoch 17, Loss: 1.380648096402486\n",
            "Generator Pretrain Epoch 18, Loss: 1.3805132706960042\n",
            "Generator Pretrain Epoch 19, Loss: 1.380361000696818\n",
            "Generator Pretrain Epoch 20, Loss: 1.3802046378453572\n",
            "Generator Pretrain Epoch 21, Loss: 1.3800246318181355\n",
            "Generator Pretrain Epoch 22, Loss: 1.3798224528630574\n",
            "Generator Pretrain Epoch 23, Loss: 1.379616141319275\n",
            "Generator Pretrain Epoch 24, Loss: 1.3794035911560059\n",
            "Generator Pretrain Epoch 25, Loss: 1.3791731595993042\n",
            "Generator Pretrain Epoch 26, Loss: 1.3788713614145915\n",
            "Generator Pretrain Epoch 27, Loss: 1.378507137298584\n",
            "Generator Pretrain Epoch 28, Loss: 1.3781508207321167\n",
            "Generator Pretrain Epoch 29, Loss: 1.377793272336324\n",
            "Generator Pretrain Epoch 30, Loss: 1.3774043718973796\n",
            "Generator Pretrain Epoch 31, Loss: 1.3769516944885254\n",
            "Generator Pretrain Epoch 32, Loss: 1.3764023780822754\n",
            "Generator Pretrain Epoch 33, Loss: 1.3758269945780437\n",
            "Generator Pretrain Epoch 34, Loss: 1.3750758568445842\n",
            "Generator Pretrain Epoch 35, Loss: 1.3743672768274944\n",
            "Generator Pretrain Epoch 36, Loss: 1.3735252221425374\n",
            "Generator Pretrain Epoch 37, Loss: 1.3733926216761272\n",
            "Generator Pretrain Epoch 38, Loss: 1.3734033107757568\n",
            "Generator Pretrain Epoch 39, Loss: 1.3747929334640503\n",
            "Generator Pretrain Epoch 40, Loss: 1.3747828404108684\n",
            "Generator Pretrain Epoch 41, Loss: 1.3741223414738972\n",
            "Generator Pretrain Epoch 42, Loss: 1.3734863996505737\n",
            "Generator Pretrain Epoch 43, Loss: 1.3726729551951091\n",
            "Generator Pretrain Epoch 44, Loss: 1.372373898824056\n",
            "Generator Pretrain Epoch 45, Loss: 1.3711896737416585\n",
            "Generator Pretrain Epoch 46, Loss: 1.3703928391138713\n",
            "Generator Pretrain Epoch 47, Loss: 1.3696619669596355\n",
            "Generator Pretrain Epoch 48, Loss: 1.368727684020996\n",
            "Generator Pretrain Epoch 49, Loss: 1.368693192799886\n",
            "Generator Pretrain Epoch 50, Loss: 1.3682100375493367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pretrain Discriminator**"
      ],
      "metadata": {
        "id": "wSMomBze7sMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretrain discriminator\n",
        "for epoch in range(D_PRETRAIN_EPOCHS):\n",
        "    for i in range(0, len(train_data), BATCH_SIZE):\n",
        "        batch = train_data[i:i+BATCH_SIZE]\n",
        "        d_optimizer.zero_grad()\n",
        "        # Real sequences\n",
        "        real_labels = torch.ones(batch.size(0), 1)\n",
        "        real_preds = discriminator(batch)\n",
        "        real_loss = criterion(real_preds, real_labels)\n",
        "        # Generate fake sequences\n",
        "        noise = torch.randint(0, VOCAB_SIZE, (batch.size(0), SEQ_LENGTH)).long()\n",
        "        fake_seq = torch.zeros(batch.size(0), SEQ_LENGTH, VOCAB_SIZE)\n",
        "        fake_seq.scatter_(2, noise.unsqueeze(-1), 1)\n",
        "        fake_labels = torch.zeros(batch.size(0), 1)\n",
        "        fake_preds = discriminator(fake_seq)\n",
        "        fake_loss = criterion(fake_preds, fake_labels)\n",
        "        loss = (real_loss + fake_loss) / 2\n",
        "        loss.backward()\n",
        "        d_optimizer.step()\n",
        "    print(f'Discriminator Pretrain Epoch {epoch+1}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "id": "9Bf3nvRH6kk_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f9df3e3-dfa3-49aa-89de-49fef809cc37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discriminator Pretrain Epoch 1, Loss: 0.6892892122268677\n",
            "Discriminator Pretrain Epoch 2, Loss: 0.6638418436050415\n",
            "Discriminator Pretrain Epoch 3, Loss: 0.6594439744949341\n",
            "Discriminator Pretrain Epoch 4, Loss: 0.5913381576538086\n",
            "Discriminator Pretrain Epoch 5, Loss: 0.5629907846450806\n",
            "Discriminator Pretrain Epoch 6, Loss: 0.5201982259750366\n",
            "Discriminator Pretrain Epoch 7, Loss: 0.4893595576286316\n",
            "Discriminator Pretrain Epoch 8, Loss: 0.4469603896141052\n",
            "Discriminator Pretrain Epoch 9, Loss: 0.47076040506362915\n",
            "Discriminator Pretrain Epoch 10, Loss: 0.374267041683197\n",
            "Discriminator Pretrain Epoch 11, Loss: 0.3711191415786743\n",
            "Discriminator Pretrain Epoch 12, Loss: 0.3366257846355438\n",
            "Discriminator Pretrain Epoch 13, Loss: 0.3141634464263916\n",
            "Discriminator Pretrain Epoch 14, Loss: 0.3903343081474304\n",
            "Discriminator Pretrain Epoch 15, Loss: 0.25405827164649963\n",
            "Discriminator Pretrain Epoch 16, Loss: 0.26806288957595825\n",
            "Discriminator Pretrain Epoch 17, Loss: 0.253582626581192\n",
            "Discriminator Pretrain Epoch 18, Loss: 0.20679481327533722\n",
            "Discriminator Pretrain Epoch 19, Loss: 0.1756030023097992\n",
            "Discriminator Pretrain Epoch 20, Loss: 0.12939079105854034\n",
            "Discriminator Pretrain Epoch 21, Loss: 0.20642122626304626\n",
            "Discriminator Pretrain Epoch 22, Loss: 0.13555893301963806\n",
            "Discriminator Pretrain Epoch 23, Loss: 0.1597251296043396\n",
            "Discriminator Pretrain Epoch 24, Loss: 0.14815115928649902\n",
            "Discriminator Pretrain Epoch 25, Loss: 0.10968270897865295\n",
            "Discriminator Pretrain Epoch 26, Loss: 0.12708815932273865\n",
            "Discriminator Pretrain Epoch 27, Loss: 0.08320974558591843\n",
            "Discriminator Pretrain Epoch 28, Loss: 0.16748276352882385\n",
            "Discriminator Pretrain Epoch 29, Loss: 0.11904510855674744\n",
            "Discriminator Pretrain Epoch 30, Loss: 0.059041254222393036\n",
            "Discriminator Pretrain Epoch 31, Loss: 0.06803879886865616\n",
            "Discriminator Pretrain Epoch 32, Loss: 0.060355547815561295\n",
            "Discriminator Pretrain Epoch 33, Loss: 0.060406915843486786\n",
            "Discriminator Pretrain Epoch 34, Loss: 0.08040912449359894\n",
            "Discriminator Pretrain Epoch 35, Loss: 0.07015611231327057\n",
            "Discriminator Pretrain Epoch 36, Loss: 0.04851668328046799\n",
            "Discriminator Pretrain Epoch 37, Loss: 0.08972190320491791\n",
            "Discriminator Pretrain Epoch 38, Loss: 0.05004433915019035\n",
            "Discriminator Pretrain Epoch 39, Loss: 0.03611333668231964\n",
            "Discriminator Pretrain Epoch 40, Loss: 0.11119376868009567\n",
            "Discriminator Pretrain Epoch 41, Loss: 0.03704501688480377\n",
            "Discriminator Pretrain Epoch 42, Loss: 0.11589911580085754\n",
            "Discriminator Pretrain Epoch 43, Loss: 0.07708172500133514\n",
            "Discriminator Pretrain Epoch 44, Loss: 0.027336612343788147\n",
            "Discriminator Pretrain Epoch 45, Loss: 0.08723805844783783\n",
            "Discriminator Pretrain Epoch 46, Loss: 0.05212552845478058\n",
            "Discriminator Pretrain Epoch 47, Loss: 0.032734643667936325\n",
            "Discriminator Pretrain Epoch 48, Loss: 0.03885297477245331\n",
            "Discriminator Pretrain Epoch 49, Loss: 0.04220999777317047\n",
            "Discriminator Pretrain Epoch 50, Loss: 0.02498641610145569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Adversial Training**"
      ],
      "metadata": {
        "id": "IfGhV-8d7kVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adversarial Training\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    for i in range(0, len(train_data), BATCH_SIZE):\n",
        "        batch = train_data[i:i+BATCH_SIZE]\n",
        "        # Train discriminator\n",
        "        d_optimizer.zero_grad()\n",
        "        real_labels = torch.ones(batch.size(0), 1)\n",
        "        real_preds = discriminator(batch)\n",
        "        real_loss = criterion(real_preds, real_labels)\n",
        "        # Generate sequences\n",
        "        noise = torch.randint(0, VOCAB_SIZE, (batch.size(0), SEQ_LENGTH)).long()\n",
        "        fake_seq = torch.zeros(batch.size(0), SEQ_LENGTH, VOCAB_SIZE)\n",
        "        fake_seq.scatter_(2, noise.unsqueeze(-1), 1)\n",
        "        probs, _ = generator(fake_seq)\n",
        "        dist = Categorical(probs)\n",
        "        gen_seq = dist.sample()\n",
        "        fake_seq = torch.zeros(batch.size(0), SEQ_LENGTH, VOCAB_SIZE)\n",
        "        fake_seq.scatter_(2, gen_seq.unsqueeze(-1), 1)\n",
        "        fake_labels = torch.zeros(batch.size(0), 1)\n",
        "        fake_preds = discriminator(fake_seq)\n",
        "        fake_loss = criterion(fake_preds, fake_labels)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "\n",
        "        # Train generator with REINFORCE\n",
        "        g_optimizer.zero_grad()\n",
        "        rewards = rollout(generator, gen_seq)\n",
        "        probs, _ = generator(fake_seq)\n",
        "        dist = Categorical(probs)\n",
        "        log_probs = dist.log_prob(gen_seq)\n",
        "        g_loss = -torch.mean(log_probs * rewards)\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "    print(f'Adversarial Epoch {epoch+1}, D Loss: {d_loss.item()}, G Loss: {g_loss.item()}')"
      ],
      "metadata": {
        "id": "7wWvMCz86xrg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e571bc28-4407-4e55-8ea6-f8e3f7be081c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adversarial Epoch 1, D Loss: 0.12251493334770203, G Loss: 0.10263233631849289\n",
            "Adversarial Epoch 2, D Loss: 0.13568075001239777, G Loss: 0.10845666378736496\n",
            "Adversarial Epoch 3, D Loss: 0.04440082609653473, G Loss: 0.013211316429078579\n",
            "Adversarial Epoch 4, D Loss: 0.04993482306599617, G Loss: 0.12127697467803955\n",
            "Adversarial Epoch 5, D Loss: 0.042251456528902054, G Loss: 0.015261474996805191\n",
            "Adversarial Epoch 6, D Loss: 0.047793373465538025, G Loss: 0.008318142034113407\n",
            "Adversarial Epoch 7, D Loss: 0.01615300215780735, G Loss: 0.04711707681417465\n",
            "Adversarial Epoch 8, D Loss: 0.005504402332007885, G Loss: 0.08652516454458237\n",
            "Adversarial Epoch 9, D Loss: 0.00948171317577362, G Loss: 0.011658132076263428\n",
            "Adversarial Epoch 10, D Loss: 0.03556269407272339, G Loss: 0.0048830704763531685\n",
            "Adversarial Epoch 11, D Loss: 0.009837744757533073, G Loss: 0.030247816815972328\n",
            "Adversarial Epoch 12, D Loss: 0.024226395413279533, G Loss: 0.020878203213214874\n",
            "Adversarial Epoch 13, D Loss: 0.009755018167197704, G Loss: 0.003372779581695795\n",
            "Adversarial Epoch 14, D Loss: 0.008059755899012089, G Loss: 0.01026471983641386\n",
            "Adversarial Epoch 15, D Loss: 0.0017510374309495091, G Loss: 0.028412949293851852\n",
            "Adversarial Epoch 16, D Loss: 0.031174639239907265, G Loss: 0.007056443486362696\n",
            "Adversarial Epoch 17, D Loss: 0.02247660607099533, G Loss: 0.006081089843064547\n",
            "Adversarial Epoch 18, D Loss: 0.01307249628007412, G Loss: 0.01703074760735035\n",
            "Adversarial Epoch 19, D Loss: 0.005515262484550476, G Loss: 0.02174057811498642\n",
            "Adversarial Epoch 20, D Loss: 0.02820601686835289, G Loss: 0.009089061990380287\n",
            "Adversarial Epoch 21, D Loss: 0.010353383608162403, G Loss: 0.009541396051645279\n",
            "Adversarial Epoch 22, D Loss: 0.026389088481664658, G Loss: 0.017091931775212288\n",
            "Adversarial Epoch 23, D Loss: 0.0054946825839579105, G Loss: 0.01315053179860115\n",
            "Adversarial Epoch 24, D Loss: 0.00424743490293622, G Loss: 0.014877062290906906\n",
            "Adversarial Epoch 25, D Loss: 0.02093939483165741, G Loss: 0.012327919714152813\n",
            "Adversarial Epoch 26, D Loss: 0.0295637845993042, G Loss: 0.007189491763710976\n",
            "Adversarial Epoch 27, D Loss: 0.005626135505735874, G Loss: 0.009632461704313755\n",
            "Adversarial Epoch 28, D Loss: 0.006834175903350115, G Loss: 0.006762518547475338\n",
            "Adversarial Epoch 29, D Loss: 0.004333562217652798, G Loss: 0.010559473186731339\n",
            "Adversarial Epoch 30, D Loss: 0.0041143642738461494, G Loss: 0.006839816924184561\n",
            "Adversarial Epoch 31, D Loss: 0.010357157327234745, G Loss: 0.00947067141532898\n",
            "Adversarial Epoch 32, D Loss: 0.006578021217137575, G Loss: 0.009763753041625023\n",
            "Adversarial Epoch 33, D Loss: 0.006764375139027834, G Loss: 0.008996124379336834\n",
            "Adversarial Epoch 34, D Loss: 0.012965462170541286, G Loss: 0.002407684223726392\n",
            "Adversarial Epoch 35, D Loss: 0.004624501336365938, G Loss: 0.021985750645399094\n",
            "Adversarial Epoch 36, D Loss: 0.10312623530626297, G Loss: 0.004556223284453154\n",
            "Adversarial Epoch 37, D Loss: 0.009964538738131523, G Loss: 0.0030558949802070856\n",
            "Adversarial Epoch 38, D Loss: 0.022519057616591454, G Loss: 0.02175031043589115\n",
            "Adversarial Epoch 39, D Loss: 0.004283533897250891, G Loss: 0.00043081570765934885\n",
            "Adversarial Epoch 40, D Loss: 0.006412442773580551, G Loss: 0.0044097998179495335\n",
            "Adversarial Epoch 41, D Loss: 0.012115949764847755, G Loss: 0.020409731194376945\n",
            "Adversarial Epoch 42, D Loss: 0.00487300893291831, G Loss: 0.009188580326735973\n",
            "Adversarial Epoch 43, D Loss: 0.005821802653372288, G Loss: 0.00695825694128871\n",
            "Adversarial Epoch 44, D Loss: 0.002663986524567008, G Loss: 0.002364113926887512\n",
            "Adversarial Epoch 45, D Loss: 0.06648709625005722, G Loss: 0.0018831646302714944\n",
            "Adversarial Epoch 46, D Loss: 0.020698552951216698, G Loss: 0.00035410813870839775\n",
            "Adversarial Epoch 47, D Loss: 0.005200603045523167, G Loss: 0.00799486879259348\n",
            "Adversarial Epoch 48, D Loss: 0.0031512510031461716, G Loss: 0.019323037937283516\n",
            "Adversarial Epoch 49, D Loss: 0.004706200212240219, G Loss: 0.02600932866334915\n",
            "Adversarial Epoch 50, D Loss: 0.0026826069224625826, G Loss: 0.0033648854587227106\n",
            "Adversarial Epoch 51, D Loss: 0.012777730822563171, G Loss: 0.0005753402947448194\n",
            "Adversarial Epoch 52, D Loss: 0.005297604016959667, G Loss: 0.016103725880384445\n",
            "Adversarial Epoch 53, D Loss: 0.00935754831880331, G Loss: 0.007404533680528402\n",
            "Adversarial Epoch 54, D Loss: 0.0025267773307859898, G Loss: 0.001062357216142118\n",
            "Adversarial Epoch 55, D Loss: 0.0029194082599133253, G Loss: 0.0015105854254215956\n",
            "Adversarial Epoch 56, D Loss: 0.002530826488509774, G Loss: 0.002009201794862747\n",
            "Adversarial Epoch 57, D Loss: 0.09131577610969543, G Loss: 0.0004067817353643477\n",
            "Adversarial Epoch 58, D Loss: 0.006540959235280752, G Loss: 0.011966620571911335\n",
            "Adversarial Epoch 59, D Loss: 0.005023496691137552, G Loss: 0.04262583330273628\n",
            "Adversarial Epoch 60, D Loss: 0.003803381696343422, G Loss: 0.008533747866749763\n",
            "Adversarial Epoch 61, D Loss: 0.01384846493601799, G Loss: 0.0011114253429695964\n",
            "Adversarial Epoch 62, D Loss: 0.00298523623496294, G Loss: 0.02286115288734436\n",
            "Adversarial Epoch 63, D Loss: 0.007633939851075411, G Loss: 0.04457797855138779\n",
            "Adversarial Epoch 64, D Loss: 0.0014723013155162334, G Loss: 0.00433709193021059\n",
            "Adversarial Epoch 65, D Loss: 0.021175900474190712, G Loss: 0.0003780734841711819\n",
            "Adversarial Epoch 66, D Loss: 0.0017992310458794236, G Loss: 0.0030938221607357264\n",
            "Adversarial Epoch 67, D Loss: 0.0026089742314070463, G Loss: 0.0002763203519862145\n",
            "Adversarial Epoch 68, D Loss: 0.004694043658673763, G Loss: 0.004997400101274252\n",
            "Adversarial Epoch 69, D Loss: 0.0037444340996444225, G Loss: 0.003367058699950576\n",
            "Adversarial Epoch 70, D Loss: 0.0009198121260851622, G Loss: 0.004207074176520109\n",
            "Adversarial Epoch 71, D Loss: 0.000720864743925631, G Loss: 0.0015546117210760713\n",
            "Adversarial Epoch 72, D Loss: 0.002449908759444952, G Loss: 0.0032564529683440924\n",
            "Adversarial Epoch 73, D Loss: 0.0006089507951401174, G Loss: 0.008259453810751438\n",
            "Adversarial Epoch 74, D Loss: 0.0025541295763105154, G Loss: 0.0013465832453221083\n",
            "Adversarial Epoch 75, D Loss: 0.0030433377251029015, G Loss: 0.004186891485005617\n",
            "Adversarial Epoch 76, D Loss: 0.0031932778656482697, G Loss: 0.0017187867779284716\n",
            "Adversarial Epoch 77, D Loss: 0.001172172836959362, G Loss: 0.0010495444294065237\n",
            "Adversarial Epoch 78, D Loss: 0.001269339700229466, G Loss: 0.004585954360663891\n",
            "Adversarial Epoch 79, D Loss: 0.008093498647212982, G Loss: 0.005605436861515045\n",
            "Adversarial Epoch 80, D Loss: 0.0008946426096372306, G Loss: 0.0009088640799745917\n",
            "Adversarial Epoch 81, D Loss: 0.0018597460584715009, G Loss: 0.0016604228876531124\n",
            "Adversarial Epoch 82, D Loss: 0.03998348489403725, G Loss: 0.0011422839015722275\n",
            "Adversarial Epoch 83, D Loss: 0.011867580004036427, G Loss: 0.00036799084045924246\n",
            "Adversarial Epoch 84, D Loss: 0.0009450347861275077, G Loss: 0.0037777768447995186\n",
            "Adversarial Epoch 85, D Loss: 0.01139133982360363, G Loss: 0.004838146269321442\n",
            "Adversarial Epoch 86, D Loss: 0.00019755229004658759, G Loss: 0.004096445627510548\n",
            "Adversarial Epoch 87, D Loss: 0.0010167064610868692, G Loss: 0.0024225700180977583\n",
            "Adversarial Epoch 88, D Loss: 0.001292004599235952, G Loss: 0.00017271983961109072\n",
            "Adversarial Epoch 89, D Loss: 0.0023448802530765533, G Loss: 3.514498894219287e-05\n",
            "Adversarial Epoch 90, D Loss: 0.0011026255087926984, G Loss: 0.00018219766207039356\n",
            "Adversarial Epoch 91, D Loss: 0.001272817375138402, G Loss: 5.9799072914756835e-05\n",
            "Adversarial Epoch 92, D Loss: 0.00248049502260983, G Loss: 8.127306500682607e-05\n",
            "Adversarial Epoch 93, D Loss: 0.0008842334500513971, G Loss: 0.0005401765229180455\n",
            "Adversarial Epoch 94, D Loss: 0.0003966346266679466, G Loss: 0.0009950289968401194\n",
            "Adversarial Epoch 95, D Loss: 0.007519779726862907, G Loss: 0.0002636475837789476\n",
            "Adversarial Epoch 96, D Loss: 0.0005370749277062714, G Loss: 0.00014893589832354337\n",
            "Adversarial Epoch 97, D Loss: 0.0008154618553817272, G Loss: 0.00021803718118462712\n",
            "Adversarial Epoch 98, D Loss: 0.0007077431655488908, G Loss: 0.00028843633481301367\n",
            "Adversarial Epoch 99, D Loss: 0.0022558593191206455, G Loss: 0.0008215623092837632\n",
            "Adversarial Epoch 100, D Loss: 0.0053290147334337234, G Loss: 0.0005269658868201077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Generate sample sequences**"
      ],
      "metadata": {
        "id": "-QQ9qNdI7cWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate sample sequences\n",
        "def generate_sequences(generator, num_samples=10):\n",
        "    generator.eval()\n",
        "    sequences = []\n",
        "    with torch.no_grad():\n",
        "        noise = torch.randint(0, VOCAB_SIZE, (num_samples, SEQ_LENGTH)).long()\n",
        "        seq = torch.zeros(num_samples, SEQ_LENGTH, VOCAB_SIZE)\n",
        "        seq.scatter_(2, noise.unsqueeze(-1), 1)\n",
        "        probs, _ = generator(seq)\n",
        "        dist = Categorical(probs)\n",
        "        gen_seq = dist.sample()\n",
        "        for seq in gen_seq:\n",
        "            seq_str = ''.join([INV_NUCLEOTIDE_MAP[idx.item()] for idx in seq])\n",
        "            sequences.append(seq_str)\n",
        "    return sequences"
      ],
      "metadata": {
        "id": "wQEpOvTa62rJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Generate and print sequences**"
      ],
      "metadata": {
        "id": "rLZRYssM7BhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate and print sequences\n",
        "generated = generate_sequences(generator)\n",
        "for seq in generated:\n",
        "    print(f'Generated Sequence: {seq}')"
      ],
      "metadata": {
        "id": "-k7iUoLX64-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62a2602f-e121-4227-bcea-54551b74a507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Sequence: TAATTAGTGCTGTATAACGTCTGAGAAATGATACGTGGCTAAGATGGATAGAAGGT\n",
            "Generated Sequence: CAATCTTGGAAAATGGAACAGGAAAAGGAGTTTAGTCATTAATTTAACAAGACAAA\n",
            "Generated Sequence: CAAAGGATTGGTTTAGTGTCAGGATACTGGTAAATAAAGTGAGATACACTAAGGGA\n",
            "Generated Sequence: GGAGATAAGATTTATGTGGTGCGAATAGATATGAGGTGAGAAGAGAGGAGAGTTCA\n",
            "Generated Sequence: CCAGCAAATGCTTCGATTGGTAACAGGGTAAGTTACTTCTAAAGGCTGAAAAGTGA\n",
            "Generated Sequence: TCAAGCAAGAGATATGCAATTTCTGTTGATAGAGAATGGTGGACCGTGGTTTTGTT\n",
            "Generated Sequence: GATAGTAATAAAGTTAAAGGATTAGGGGAACTAAGTGTTTGTCGATAACGGGTATA\n",
            "Generated Sequence: TCGAAGAATGAGAAGCAACTGGATTATTAGGCATCACAATAAATAGTGGAAGGAGA\n",
            "Generated Sequence: GTAATCGGGGGCCTTAGCCGATCGGAAAATAAAGTGGTACATAATTAATATCATAA\n",
            "Generated Sequence: GCCTAAAAGGTCAGATTTGGTAGAGGGAGTTTATATTTGCCACGAATGAATGTGTT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(generator, 'generator.pth')"
      ],
      "metadata": {
        "id": "sTDtYR7_OlR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(discriminator, 'discriminator.pth')"
      ],
      "metadata": {
        "id": "J94F50rqPTIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(g_optimizer, 'g_optimizer.pth')"
      ],
      "metadata": {
        "id": "uxodCxrAPge6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(g_optimizer, 'd_optimizer.pth')"
      ],
      "metadata": {
        "id": "rkq0KS2GQGVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(generator.state_dict, 'generator_state_dict.pth')\n",
        "torch.save(discriminator.state_dict, 'discriminator_state_dict.pth')\n",
        "torch.save(g_optimizer.state_dict, 'g_optimizer_state_dict.pth')\n",
        "torch.save(d_optimizer.state_dict, 'd_optimizer_state_dict.pth')"
      ],
      "metadata": {
        "id": "1YKnYHN3QkKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**"
      ],
      "metadata": {
        "id": "m6m2n2OB7I8M"
      }
    }
  ]
}
